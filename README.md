# Домашнее задание к уроку 4: Сверточные сети
## Цель задания
### Сравнить эффективность сверточных и полносвязных сетей на задачах компьютерного зрения, изучить преимущества CNN архитектур.

## Задание 1: Сравнение CNN и полносвязных сетей 

### 1.1 Сравнение на MNIST
Сравните производительность на MNIST:
- Полносвязная сеть (3-4 слоя)
- Простая CNN (2-3 conv слоя)
- CNN с Residual Block

Для каждого варианта:
- Обучите модель с одинаковыми гиперпараметрами
- Сравните точность на train и test множествах
- Измерьте время обучения и инференса
- Визуализируйте кривые обучения
- Проанализируйте количество параметров

### Описание:

1. Модели:

- Полносвязная сеть (FCNet) - базовый вариант
- Простая CNN (SimpleCNN) - свёрточная сеть с двумя слоями
- ResNet - более сложная архитектура с остаточными связями

2. Процесс:

- Загрузка и подготовка данных (нормализация)
- Обучение каждой модели по 10 эпох
- Оценка точности на тренировочных и тестовых данных
- Замер времени обучения и времени предсказания
- Подсчёт количества параметров каждой модели

3. Визуализация:

- Графики потерь и точности во время обучения
- Графики времени обучения на каждой эпохе

### Пример: 

1. FCMet Training Loss & Accuracy

- Аналогичные графики для другой модели (FCMet).
- Общая точность (Total Accuracy) выделена, но детали неясны из-за фрагментарности данных.

2. SimpleCMM Training Loss & Accuracy

- Потери снижаются с 0.14 до ~0.02 за 8 эпох.
- Test Accuracy достигает ~97%, что выше, чем у других моделей.
- Время обучения (Training Time) варьируется от 26.6 до 36.5 условных единиц.

Лучшая модель — SimpleCMM (высокая точность 97%, низкие потери)

![image](https://github.com/user-attachments/assets/6ed0fad8-b4a4-425c-855e-957f833297a6)

![image](https://github.com/user-attachments/assets/b22c4b3e-3fb0-4ced-9f16-2b9fd9aa2ac8)


Результаты:

- Вывод итоговых метрик для всех моделей
- Сравнение точности, скорости и сложности моделей

### 1.2 Сравнение на CIFAR-10 
Сравните производительность на CIFAR-10:
- Полносвязная сеть (глубокая)
- CNN с Residual блоками
- CNN с регуляризацией и Residual блоками
 
Для каждого варианта:
- Обучите модель с одинаковыми гиперпараметрами
- Сравните точность и время обучения
- Проанализируйте переобучение
- Визуализируйте confusion matrix
- Исследуйте градиенты (gradient flow)

Цель задачи: сравнить, как разные архитектуры (от простой FCN до сложной ResNet) справляются с задачей классификации, учитывая не только точность, но и устойчивость обучения.

1. Модели:

- DeepFCN — глубокая полносвязная сеть (5 слоёв) с dropout для регуляризации.
- ResNet — свёрточная сеть с остаточными блоками (Residual Blocks) для борьбы с исчезающими градиентами.
- RegularizedResNet — улучшенная версия ResNet с дополнительным dropout и L2-регуляризацией.

2. Процесс:

- Загрузка и аугментация данных CIFAR-10 (рандомные crop/flip + нормализация).
- Обучение каждой модели по 50 эпох с замером времени.
- Использование Adam-оптимизатора с L2-регуляризацией.
- Оценка точности на тренировочных и тестовых данных.

3. Визуализация:

- Графики потерь и точности.
- Confusion matrix для анализа ошибок.
- Распределение градиентов по слоям (для выявления проблем обучения).

Примеры выводов: 

1. DeepFCN Training Loss & Accuracy

- Графики потерь и точности (train/test) в зависимости от эпох
- Test Accuracy ниже Train Accuracy, что указывает на переобучение

![image](https://github.com/user-attachments/assets/b4e8d4f6-f054-4468-8d5a-5058a8be76e0)

2. Confusion Matrix

- Матрица ошибок для 10 классов (например, airplane, automobile и др.)
- Наибольшая точность у классов truck (753) и ship (565), худшая — у bird (24) и cat (133)

![image](https://github.com/user-attachments/assets/f0feb710-ca68-4f9e-8b96-9b5c7a1f6200)

3. Gradient Flow
- Показывает средний градиент по слоям нейросети
- Градиенты уменьшаются от первого слоя к последующим
  
![image](https://github.com/user-attachments/assets/900c8408-f633-40db-a7a3-d49ce8c8de48)

4. Результаты:

- Вывод итоговых метрик: точность на train/test, время обучения, число параметров.
- Сравнение моделей по эффективности и устойчивости к переобучению.

## Задание 2: Анализ архитектур CNN 

### 2.1 Влияние размера ядра свертки 
Исследуйте влияние размера ядра свертки:
- 3x3 ядра
- 5x5 ядра
- 7x7 ядра
- Комбинация разных размеров (1x1 + 3x3)

Для каждого варианта:
- Поддерживайте одинаковое количество параметров
- Сравните точность и время обучения
- Проанализируйте рецептивные поля
- Визуализируйте активации первого слоя

Описание: 

1. Архитектуры моделей:

- ConvNet - базовая CNN с настраиваемыми ядрами свертки (принимает разные размеры ядер)
- MixedConvNet - комбинирует 1x1 и 3x3 свертки в первом слое

2. Экспериментальные конфигурации:

- 3x3 ядра (стандартный выбор)
- 5x5 ядра (большее рецептивное поле)
- 7x7 ядра (еще большее рецептивное поле)
- Комбинация 1x1+3x3 ядер (mixed)

Пример вывода: 

![image](https://github.com/user-attachments/assets/a3a17bd8-1673-4d30-aa15-69fee4c2c1fd)

![image](https://github.com/user-attachments/assets/54b59762-e33a-4fe9-80c9-74491b8212ba)

3. Ключевые особенности:

- Анализ рецептивных полей для каждой конфигурации
- Визуализация активаций первого слоя
- Сравнение точности и времени обучения
- Batch Normalization после каждого сверточного слоя
- Адаптивная подготовка данных с нормализацией

![image](https://github.com/user-attachments/assets/98f13a98-dc1f-4a36-8639-3aafcb26fdcf)

4. Метрики сравнения:

- Точность на тренировочном и тестовом наборах
- Время обучения
- Размер рецептивного поля
- Количество параметров модели

5. Визуализации:

- Активации первого сверточного слоя (первые 16 фильтров)
- Сравнительные графики точности и времени обучения

Результат: Код представляет собой систематическое исследование одного из ключевых гиперпараметров CNN - размера ядра свертки
  
### 2.2 Влияние глубины CNN 
Исследуйте влияние глубины CNN:
- Неглубокая CNN (2 conv слоя)
- Средняя CNN (4 conv слоя)
- Глубокая CNN (6+ conv слоев)
- CNN с Residual связями

Для каждого варианта:
- Сравните точность и время обучения
- Проанализируйте vanishing/exploding gradients
- Исследуйте эффективность Residual связей
- Визуализируйте feature maps

Описание: 

1. Архитектуры моделей:

- ShallowCNN (2 сверточных слоя) - базовая неглубокая сеть
- MediumCNN (4 слоя) - промежуточный вариант
- DeepCNN (6 слоев) - глубокая сеть
- ResNet (с остаточными связями) - устойчивая к проблеме исчезающих градиентов

2. Ключевые этапы работы:

Подготовка данных:

- Загрузка CIFAR-10 с нормализацией
- Разделение на train/test (50k/10k изображений 32x32)

Обучение моделей (10 эпох):

- Использование Adam оптимизатора
- CrossEntropyLoss
- Анализ градиентов (нормы, vanishing/exploding)
- Визуализация feature maps

Метрики сравнения:

- Точность на train/test
- Время обучения
- Количество параметров
- Динамика градиентов

3. Особенности реализации:

- Автоматический расчет размеров тензоров
- Batch Normalization после каждого сверточного слоя
- Адаптивный пулинг в ResNet
- Хуки для визуализации активаций
- Логирование норм градиентов

4. Визуализации:

- Feature maps первых слоев
- Сравнительные графики:

  - Точность моделей
  - Время обучения
  - Нормы градиентов
 
Примеры вывода: 

![image](https://github.com/user-attachments/assets/ab2f3e09-a52f-4238-be49-7d459dcf6787)

![image](https://github.com/user-attachments/assets/2704cd4d-4957-495e-8925-da868e2dd5eb)


5. Результаты:

- ResNet показывает лучшую точность благодаря residual-связям
- Более глубокие сети требуют больше времени обучения
- Градиенты в DeepCNN могут демонстрировать проблемы vanishing/exploding
- ShallowCNN быстрее всего обучается, но имеет меньшую точность

## Задание 3: Кастомные слои и эксперименты 

### 3.1 Реализация кастомных слоев 
Реализуйте кастомные слои:
- Кастомный сверточный слой с дополнительной логикой
- Attention механизм для CNN
- Кастомная функция активации
- Кастомный pooling слой

Для каждого слоя:
- Реализуйте forward и backward проходы
- Добавьте параметры если необходимо
- Протестируйте на простых примерах
- Сравните с стандартными аналогами

Описпние: 

1. NoisyConv2d - кастомный сверточный слой:

- Добавляет learnable шум к входным данным во время обучения
- Масштаб шума - обучаемый параметр
- Полезен для регуляризации и увеличения robustness модели

Пример вывода: 

![image](https://github.com/user-attachments/assets/44730c7a-1b8c-41fd-9c35-abfcdbf54292)

2. ChannelAttention - механизм внимания по каналам:

- Комбинирует avg и max pooling для создания channel-wise attention карты
- Использует MLP с bottleneck архитектурой (reduction_ratio)
- Автоматически определяет важные каналы признаков

3. Swish - кастомная функция активации:

- Swish(x) = x * sigmoid(beta * x)
- Параметр beta - обучаемый
- Нелинейность с плавным переходом, показавшая хорошие результаты на глубоких сетях

4. HybridPool2d - гибридный pooling слой:

- Комбинация average и max pooling
- Коэффициент смешивания alpha - обучаемый параметр
- Объединяет преимущества обоих типов пулинга

Код включает тестирование каждого слоя:

- Проверка размерностей входов/выходов
- Визуализация работы (шум, функции активации)
- Сравнение с обычными аналогами

Пример вывода: 

![image](https://github.com/user-attachments/assets/075a12e7-0d27-4764-a4a8-0bd6c3dd86e0)

Также реализована CustomCNN - демонстрационная модель, объединяющая все кастомные слои:

- 2 блока (NoisyConv2d → ChannelAttention → Swish → HybridPool2d)
- Полносвязный слой на выходе

Особенности реализации:

- Для Swish использован кастомный Function класс для корректного обратного распространения
- Все обучаемые параметры (noise_scale, beta, alpha) инициализируются разумными значениями
- Визуализации помогают понять работу каждого компонента
- Полная модель проверяется на прямом и обратном проходе
  
### 3.2 Эксперименты с Residual блоками 
Исследуйте различные варианты Residual блоков:
- Базовый Residual блок
- Bottleneck Residual блок
- Wide Residual блок

Для каждого варианта:
- Реализуйте блок с нуля
- Сравните производительность
- Проанализируйте количество параметров
- Исследуйте стабильность обучения

Описание: 

1. Архитектуры Residual блоков:

- BasicBlock - стандартный residual блок с двумя 3x3 свертками
- BottleneckBlock - блок с "бутылочным горлышком" (1x1-3x3-1x1 свертки)
- WideBlock - широкая версия residual блока с dropout регуляризацией

2. Общая архитектура ResNet:

- Поддерживает все три типа residual блоков
- Реализует стандартную схему ResNet с тремя основными слоями
- Поддержка "широких" сетей через параметр widen_factor

3. Экспериментальная часть:

- Сравнение трех моделей: BasicResNet, BottleneckResNet и WideResNet
- Использование SGD с моментом и L2-регуляризацией
- Планировщик обучения с уменьшением LR на 30 и 40 эпохах
- Анализ градиентов в процессе обучения

4. Визуализации:

- Графики потерь и точности на train/test
- Динамика норм градиентов
- Сравнительные графики точности и количества параметров

5. Особенности реализации:

- Подробная обработка данных CIFAR-10 с аугментацией
- Корректная обработка shortcut connections в residual блоках
- Поддержка разного числа каналов между блоками через атрибут expansion
- Комплексный анализ обучения через мониторинг градиентов

6. Ключевые выводы, которые можно сделать из этого кода:

Разные типы residual блоков предлагают компромисс между:

- Количеством параметров (Bottleneck самый компактный)
- Скоростью обучения
- Финальной точностью

WideResNet с dropout демонстрирует:

- Лучшую регуляризацию
- Более стабильное обучение
- Хороший баланс между точностью и числом параметров

Мониторинг градиентов помогает:

- Выявлять проблемы vanishing/exploding gradients
- Настраивать параметры оптимизации

## Вывод:

Лучшие результаты показали ResNet-подобные архитектуры с residual-связями, batch normalization и грамотной регуляризацией. Они обеспечивают стабильное обучение даже в глубоких сетях и достигают точности >90% на CIFAR-10. Для более сложных датасетов можно масштабировать подход, увеличивая глубину и ширину сети.
